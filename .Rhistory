install.packages("kernsmooth")
install.packages("KernSmooth")
library(KernSmooth)
install.packages("swirl")
library(swirl)
swirl()
bye()
library("swirl")
swirl()
1:20
pi:20
pi:10
15:1
?`:`
seq(1,20)
seq(1,20, by=0.5)
seq(1,10, by=0.5)
seq(0,10, by=0.5)
my_seq<-seq(5,10, length=30)
length(my_seq)
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0, times = 40)
rep(c(0,1,2), times = 10)
rep(c(0,1,2), each = 10)
library(swirl)
swirl()
num_vect<-c(0.5,55,-10,6)
tf<-num_vect<1
tf
num_vect>=6
c("My","name","is")
my_char<-c("My","name","is")
my_char
paste(my_char,collapse = " ")
my_name<- c(my_char, "Hannah")
my_name
paste(my_name,collapse = " ")
paste("Hello", "world!", sep = " ")
paste(1:3,c("X", "Y", "Z"), sep = "")
paste(LETTERS, 1:4, sep = "-")
1
library(datasets)
data(iris)
?iris
mean(iris[,1])
str(iris)
head(iris)
colMeans(iris)
apply(iris[, 1:4], 1, mean)
apply(iris[, 1:4], 2, mean)
apply(iris, 2, mean)
library(datasets)
data(mtcars)
?mtcars
str(mtcars)
head(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
with(mtcars, tapply(mpg, cyl, mean))
sapply(mtcars, cyl, mean)
l<-with(mtcars, tapply(mpg, cyl, mean))
l[1,2]
l[1]
abs(l[1]-l[2])
?iris
head(iris)
?tapply
?factor
f<-factor(iris[5])
f<-as.factor(iris[5])
f<-iris[5]
f
f<-as.factor(f)
sort.list(f)
str(f)
r<-iris[,1]
fd<-f=="virginica"
l[fd]
r[fd]
mean(r[fd])
apply(iris[, 1:4], 2, mean)
with(mtcars, tapply(mpg, cyl, mean))
with(mtcars, tapply(hp, cyl, mean))
l<-with(mtcars, tapply(hp, cyl, mean))
abs(l[1]-l[3])
library(httr)
library(httpuv)
install.packages("httpuv")
library(httpuv)
library("httr", lib.loc="C:/Program Files/R/R-3.2.1/library")
myapp <- oauth_app("new application",
key = "dc339490fbf8b052078d",
secret = "59f6c10ec46ce6a0458a3955cb80f28960a855c5")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
mydata <- GET("https://api.github.com/users/jtleek/repos", gtoken)
install.packages("jsonlite")
library("jsonlite", lib.loc="C:/Program Files/R/R-3.2.1/library")
jsondata<-toJSON(mydata)
jsondata<-asJSON(mydata)
jsondata<-serializeJSON(mydata)
newdata<-fromJSON(jsondata)
newdata<-fromJSON("https://api.github.com/users/jtleek/repos")
head(newdata)
summary(newdata)
names(newdata)
newdata[newdata$name == "datasharing",]
install.packages("sqldf")
library(sqldf)
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL,destfile = "./data.csv")
list.files()
read.table("data.csv")
acs<-read.table("data.csv")
acs<-read.csv.sql("data.csv")
install.packages("tcltk")
install.packages("tcltk")
summary(acs)
sqldf("select * from acs where AGEP < 50 and pwgtp1")
library("sqldf", lib.loc="C:/Program Files/R/R-3.2.1/library")
sqldf("select * from acs where AGEP < 50 and pwgtp1")
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select distinct AGEP from acs")
con<-url("http://biostat.jhsph.edu/~jleek/contact.html ")
htmljeffleek<-readLines(con)
close(con )
sapply(htmljeffleek[c(10, 20, 30, 100)], nchar)
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileURL,"data2.for")
con = file(fileURL)
mm<-readLines(con )
close(con )
con = file(fileURL)
read.fwf(con,c(9,4,3,4,3,4,3,4,3))
mm<-read.fwf(con,c(10,4,3,4,3,4,3,4,3))
con = file(fileURL)
mm<-read.fwf(con,c(10,4,3,4,3,4,3,4,3))
head(mm)
mm<-read.fwf(con,c(10,4,3,4,3,4,3,4,3),header= TRUE)
con = file(fileURL)
mm<-read.fwf(con,c(10,4,3,4,3,4,3,4,3),header= TRUE)
con = file(fileURL)
mm<-read.fwf(con,c(9,4,3,4,3,4,3,4,3))
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(15,4,3,4,3,4,3,4,3))
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(15,4,7,4,3,4,3,4,3))
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,4,3,4,3))
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,5,9,5,3))
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,5,8,5,3))
head(mm)
close(con )
mm<-mm[-c(1,2,3),]
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,4,9,5,3))
head(mm)
mm<-read.fwf(con,c(15,4,9,4,9,4,9,4,4))
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,4,9,4,4))
mm<-mm[-c(1,2,3),]
head(mm)
mean(mm[,4])
mean(as.numeric(mm[,4]))
mean(as.numeric(mm[-c(1),4]))
mm<-mm[-c(1),]
head(mm)
mean(mm[,4])
mean(mm[,4],rm.na=TRUE)
mean(as.numeric(mm[,4]),rm.na=TRUE)
length(mm$V4)
mm[,mm$V1]<-as.numeric(mm[,mm$V1])
mm[mm$V1,]<-as.numeric(mm[mm$V1,])
mm[2,]<-as.numeric(mm[2,])
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,4,9,4,4))
mean(as.numeric(mm[,4]),rm.na=TRUE)
mm<-mm[-c(1),]
mm<-mm[-c(1),]
mm<-mm[-c(1),]
mm<-mm[-c(1),]
mean(as.numeric(mm[,4]),rm.na=TRUE)
sum(as.numeric(mm[,4]),rm.na=TRUE)
mm[,4]
con = file(fileURL)
mm<-read.fwf(con,c(15,4,9,4,9,4,9,4,4))
head(mm)
con = file(fileURL)
mm<-read.fwf(con,c(14,5,8,5,8,5,8,5,4))
mm<-mm[-c(1:4),]
head(mm)
sum(as.numeric(mm[,4]),rm.na=TRUE)
sum(as.numeric(mm[,5]),rm.na=TRUE)
mean(c(10,7,9,10,6,1,7,6,7,1))
mean(c(3,4,10,10,6,2,8,10,3,5))
median(c(4,5,4,4,3,7,9,9,6))
median(c(6,2,7,9,6))
mean(c(10,8,8,6,7))
library("dplyr", lib.loc="C:/Program Files/R/R-3.2.1/library")
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
newData <- reshape(meanstdData, direction = "long", idvar = "subject", varying = columnnames)
newData <- reshape(meanstdData, direction = "long", varying = columnnames)
newData <- reshape(meanstdData, direction = "long", varying = as.character(columnnames))
install.packages("tidyr")
library(tidyr)
stocks <- data.frame(
time = as.Date('2009-01-01') + 0:9,
X = rnorm(10, 0, 1),
Y = rnorm(10, 0, 2),
Z = rnorm(10, 0, 4)
)
gather(stocks, stock, price, -time)
stocks %>% gather(stock, price, -time)
newData <- gather(meanstdData, subject, activity, columnnames)
View(columnnames)
newData <- gather(meanstdData, subject, activity, tBodyAcc-mean():fBodyBodyGyroJerkMag-std())
newData <- gather(meanstdData, subject, activity, tBodyAcc-mean()-X:fBodyBodyGyroJerkMag-std())
names(meanstdData)<- gsub("-","_",names(meanstdData))
newData <- gather(meanstdData, subject, activity, tBodyAcc_mean()_X:fBodyBodyGyroJerkMag_std())
newData <- gather(meanstdData, subject, activity)
View(newData)
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
library("dplyr", lib.loc="C:/Program Files/R/R-3.2.1/library")
unique(meanstdData$subject)
activity_labels$V2
unique(meanstdData$activity)
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
View(meanstdData)
summarise_each(newData, funs(mean))
names(newData) <- c("subject","activity","measurement","value")
summarise_each(newData, funs(mean))
groups <- group_by(newData, subject, activity, measurement)
View(groups)
summarise_each(groups, funs(mean))
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
View(meanstdData)
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
View(newData)
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
setwd("~/datasciencecoursera")
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
source('~/datasciencecoursera/getandcleandataproject/run_analysis.R')
mean(c(8,8,1,7,8,1,7,4))
median(c(8,8,1,7,8,1,7,4))
read.csv("./data/idaho.csv")
idaho<-read.csv("./data/idaho.csv")
m<-strsplit(names(idaho),"wgtp")
m<-strsplit(names(idaho),"WGTP")
m[123]
m<-strsplit(names(idaho),"wgtp")
m[123]
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv","./data/gdp.csv")
n<-read.csv("./data/gdp.csv")
View(n)
n[-c(1,2,3,4,195:330),]
nn<-n[-c(1,2,3,4,195:330),]
View(nn)
nn<-nn[,c(1,2,4,5)]
gsub(",","",nn$X.3)
nn$X.3<-gsub(",","",nn$X.3)
mean(nn$X.3)
mean(is.numeric(nn$X.3))
mean(as.numeric(nn$X.3))
grep("^United",nn$X.2)
read.csv("./data/coutnry.csv")
read.csv("./data/country.csv")
country<-read.csv("./data/country.csv")
View(country)
country<-country[country$CountryCode %in% nn$X,]
View(country)
merge(nn,country)
gdc<-merge(nn,country)
gdc<-merge(country,nn)
names(nn)<- c("CountryCode","gdpranking","Long.Name","gdp")
gdc<-merge(country,nn)
View(gdc)
gdc<-merge(country,nn,by = CountryCode)
gdc<-merge(country,nn,by = country$CountryCode)
gdc<-merge(country,nn,by = nn$CountryCode)
gdc<-merge(country,nn,by.y  = nn$CountryCode)
names(nn)<- c("CountryCode","gdpranking","LongName","gdp")
gdc<-merge(country,nn)
View(gdc)
grepl("[Jj]une",gdc$Special.Notes)
gdc[grepl("[Jj]une",gdc$Special.Notes),]
w<-gdc[grepl("[Jj]une",gdc$Special.Notes),]
View(w)
w$Special.Notes
w<-w$Special.Notes
w
grepl("Fiscal year end: June",w)
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
table(grepl("2012",sampleTimes))
mon<-weekdays(sampleTimes)
table(mon)
two<-sampleTimes[grepl("2012",sampleTimes))]
two<-sampleTimes[grepl("2012",sampleTimes)]
mon<-weekdays(two)
table(mon)
dataurl<- "https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2Fhousehold_power_consumption.zip"
download.file(dataurl,"./epc.zip")
data<-read.table("./epc/household_power_consumption.txt", na.strings = "?")
data<-read.table("./epc/household_power_consumption.txt", na.strings = "?", sep = ";")
View(data)
data<-read.table("./epc/household_power_consumption.txt", header = TRUE, na.strings = "?", sep = ";")
View(data)
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
setwd("~/datasciencecoursera/exploratorydata1/ExData_Plotting1")
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
View(epcdata)
source('~/datasciencecoursera/exploratorydata1/ExData_Plotting1/new figures/plot1.R')
View(epcdata)
